#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•´åˆä¸‰ä¸ªæ•°æ®æºï¼Œæ„å»ºå®Œæ•´çš„çŸ¥è¯†åº“
"""

import json
import os
from typing import List, Dict
from langchain_chroma import Chroma  
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings  
import dotenv

dotenv.load_dotenv()
OPENAI_API_KEY = os.getenv("openrouter_api_key")
OPENAI_BASE_URL = os.getenv("url_openrouter")

if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY is not set")
if not OPENAI_BASE_URL:
    raise ValueError("OPENAI_BASE_URL is not set")

# é…ç½®å‚æ•°
DATA_DIR = "data"
CHROMA_DB_DIR = "chroma_db"

# æ•°æ®æ–‡ä»¶
QUERY_REQUIREMENTS_FILE = "query_business_requirements.json"
META_TABLES_FILE = "singabi_meta_tables.json"
DATA_DICTIONARY_FILE = "singabi_data_dictionary.json"

# çŸ¥è¯†åº“åç§°
QUERY_KB_NAME = "query_requirements_kb"
TABLE_KB_NAME = "meta_tables_kb"


def load_json_file(file_path: str):
    """åŠ è½½ JSON æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def build_query_requirements_kb(query_data: dict) -> List[Document]:
    """
    æ„å»ºä¸šåŠ¡éœ€æ±‚æŸ¥è¯¢çŸ¥è¯†åº“
    
    æ•´åˆå†…å®¹ï¼š
    - query id
    - query name  
    - business_requirement (ä½œä¸ºä¸»è¦æ£€ç´¢å†…å®¹)
    
    Args:
        query_data: query_business_requirements.json çš„æ•°æ®
        
    Returns:
        Document åˆ—è¡¨
    """
    documents = []
    
    for query_id, value in query_data.items():
        name = value.get("name", "")
        requirement = value.get("business_requirement", "")
        
        # æ„å»ºå®Œæ•´çš„å†…å®¹ç”¨äºæ£€ç´¢
        # åŒ…å« name å’Œ requirementï¼Œè®©å‘é‡æ£€ç´¢æ›´å‡†ç¡®
        full_content = f"æŸ¥è¯¢ID: {query_id}\næŸ¥è¯¢åç§°: {name}\n\nä¸šåŠ¡éœ€æ±‚: {requirement}"
        
        # åˆ›å»º Document å¯¹è±¡
        doc = Document(
            page_content=full_content,
            metadata={
                "query_id": query_id,
                "name": name,
                "requirement": requirement,
                "source": "query_business_requirements"
            }
        )
        documents.append(doc)
    
    print(f"âœ… ä¸šåŠ¡éœ€æ±‚çŸ¥è¯†åº“ï¼šåŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


def build_meta_tables_kb(
    meta_data: dict, 
    dict_data: list
) -> List[Document]:
    """
    æ„å»ºæ•°æ®è¡¨å…ƒæ•°æ®çŸ¥è¯†åº“
    
    æ•´åˆä¸‰ä¸ªæ•°æ®æºï¼š
    1. singabi_meta_tables.json: id, table_name, chunk_content
    2. singabi_data_dictionary.json: table_id, table_name, column_description
    
    Args:
        meta_data: singabi_meta_tables.json çš„æ•°æ®
        dict_data: singabi_data_dictionary.json çš„æ•°æ®
        
    Returns:
        Document åˆ—è¡¨
    """
    # å…ˆæ„å»º dictionary çš„æ˜ å°„è¡¨
    dict_map = {}
    for item in dict_data:
        table_name = item.get('table_name', '')
        column_desc = item.get('column_description', '')
        if table_name and column_desc:
            dict_map[table_name] = column_desc
    
    print(f"ğŸ“– æ•°æ®å­—å…¸ï¼šåŠ è½½äº† {len(dict_map)} ä¸ªå­—æ®µæè¿°")
    
    documents = []
    
    for key, value in meta_data.items():
        table_id = value.get("id", key)
        table_name = value.get("table_name", "")
        chunk_content = value.get("chunk_content", "")
        
        # ä» dictionary è·å–é¢å¤–çš„å­—æ®µæè¿°
        column_description = dict_map.get(table_name, "")
        
        # æ„å»ºå®Œæ•´çš„å†…å®¹ç”¨äºæ£€ç´¢
        # chunk_content å·²ç»å¾ˆå®Œæ•´ï¼Œå¦‚æœæœ‰é¢å¤–çš„ column_description å°±è¿½åŠ 
        full_content = chunk_content
        if column_description:
            full_content += f"\n\nè¡¥å……å­—æ®µè¯´æ˜:\n{column_description}"
        
        # åˆ›å»º Document å¯¹è±¡
        doc = Document(
            page_content=full_content,
            metadata={
                "table_id": table_id,
                "table_name": table_name,
                "chunk_content": chunk_content,
                "column_description": column_description,
                "source": "singabi_meta_tables and singabi_data_dictionary"
            }
        )
        documents.append(doc)
    
    print(f"âœ… æ•°æ®è¡¨å…ƒæ•°æ®çŸ¥è¯†åº“ï¼šåŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


def create_vector_store(
    documents: List[Document],
    collection_name: str,
    persist_directory: str
) -> Chroma:
    """
    åˆ›å»ºå‘é‡æ•°æ®åº“
    
    Args:
        documents: Document åˆ—è¡¨
        collection_name: é›†åˆåç§°
        persist_directory: æŒä¹…åŒ–ç›®å½•
        
    Returns:
        Chroma å‘é‡æ•°æ®åº“å®ä¾‹
    """
    print(f"\nğŸ”¨ æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“: {collection_name}")
    print(f"   æ–‡æ¡£æ•°é‡: {len(documents)}")
    print(f"   æŒä¹…åŒ–ç›®å½•: {persist_directory}")
    
    # åˆ›å»º Embeddings
    embeddings = OpenAIEmbeddings(
        api_key=OPENAI_API_KEY,
        base_url=OPENAI_BASE_URL,
        model="openai/text-embedding-3-large"
    )
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        collection_name=collection_name,
        persist_directory=persist_directory
    )
    
    print(f"âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼")
    return vectorstore


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ å¼€å§‹æ„å»ºå¢å¼ºç‰ˆçŸ¥è¯†åº“")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®æ–‡ä»¶
    print("\nğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶...")
    
    query_data_path = os.path.join(DATA_DIR, QUERY_REQUIREMENTS_FILE)
    meta_data_path = os.path.join(DATA_DIR, META_TABLES_FILE)
    dict_data_path = os.path.join(DATA_DIR, DATA_DICTIONARY_FILE)
    
    query_data = load_json_file(query_data_path)
    meta_data = load_json_file(meta_data_path)
    dict_data = load_json_file(dict_data_path)
    
    print(f"   âœ… {QUERY_REQUIREMENTS_FILE}: {len(query_data)} æ¡")
    print(f"   âœ… {META_TABLES_FILE}: {len(meta_data)} æ¡")
    print(f"   âœ… {DATA_DICTIONARY_FILE}: {len(dict_data)} æ¡")
    
    # 2. æ„å»ºä¸šåŠ¡éœ€æ±‚çŸ¥è¯†åº“
    print("\nğŸ“ æ„å»ºä¸šåŠ¡éœ€æ±‚çŸ¥è¯†åº“...")
    query_documents = build_query_requirements_kb(query_data)
    
    query_kb_dir = os.path.join(CHROMA_DB_DIR, QUERY_KB_NAME)
    query_vectorstore = create_vector_store(
        documents=query_documents,
        collection_name=QUERY_KB_NAME,
        persist_directory=query_kb_dir
    )
    
    # 3. æ„å»ºæ•°æ®è¡¨å…ƒæ•°æ®çŸ¥è¯†åº“ï¼ˆæ•´åˆ meta_tables å’Œ dictionaryï¼‰
    print("\nğŸ“Š æ„å»ºæ•°æ®è¡¨å…ƒæ•°æ®çŸ¥è¯†åº“...")
    table_documents = build_meta_tables_kb(meta_data, dict_data)
    
    table_kb_dir = os.path.join(CHROMA_DB_DIR, TABLE_KB_NAME)
    table_vectorstore = create_vector_store(
        documents=table_documents,
        collection_name=TABLE_KB_NAME,
        persist_directory=table_kb_dir
    )
    
    # 4. å®Œæˆ
    print("\n" + "=" * 60)
    print("ğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
    print("=" * 60)
    print(f"\nğŸ“ çŸ¥è¯†åº“ä½ç½®:")
    print(f"   1. ä¸šåŠ¡éœ€æ±‚çŸ¥è¯†åº“: {query_kb_dir}")
    print(f"   2. æ•°æ®è¡¨å…ƒæ•°æ®çŸ¥è¯†åº“: {table_kb_dir}")
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   ä¸šåŠ¡éœ€æ±‚æ–‡æ¡£æ•°: {len(query_documents)}")
    print(f"   æ•°æ®è¡¨æ–‡æ¡£æ•°: {len(table_documents)}")
    print(f"   æ€»æ–‡æ¡£æ•°: {len(query_documents) + len(table_documents)}")
    print("\nâœ¨ å¯ä»¥å¼€å§‹ä½¿ç”¨ MCP æœåŠ¡å™¨äº†ï¼")


if __name__ == "__main__":
    main()

