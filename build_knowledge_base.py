#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†åº“æ„å»ºè„šæœ¬
ä½¿ç”¨ LangChain å’Œ Chroma æ„å»ºä¸¤ä¸ªå‘é‡æ•°æ®åº“ï¼š
1. query_requirements_kb: åŸºäºä¸šåŠ¡éœ€æ±‚æŸ¥è¯¢çš„çŸ¥è¯†åº“
2. meta_tables_kb: åŸºäºæ•°æ®è¡¨å…ƒæ•°æ®çš„çŸ¥è¯†åº“
"""

import json
import os
from typing import List, Dict
from langchain_chroma import Chroma  
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings  
import dotenv
dotenv.load_dotenv()
OPENAI_API_KEY = os.getenv("openrouter_api_key")
OPENAI_BASE_URL = os.getenv("url_openrouter")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY is not set")
if not OPENAI_BASE_URL:
    raise ValueError("OPENAI_BASE_URL is not set")

# é…ç½®å‚æ•°
DATA_DIR = "data"
CHROMA_DB_DIR = "chroma_db"
QUERY_REQUIREMENTS_FILE = "query_business_requirements.json"
META_TABLES_FILE = "singabi_meta_tables.json"

# çŸ¥è¯†åº“åç§°
QUERY_KB_NAME = "query_requirements_kb"
TABLE_KB_NAME = "meta_tables_kb"


def load_json_file(file_path: str) -> dict:
    """åŠ è½½ JSON æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def build_query_requirements_kb(data: dict) -> List[Document]:
    """
    æ„å»ºä¸šåŠ¡éœ€æ±‚æŸ¥è¯¢çŸ¥è¯†åº“
    å°†æ¯ä¸ª business_requirement ä½œä¸º chunkï¼Œid å’Œ name ä½œä¸ºå…ƒæ•°æ®
    
    Args:
        data: query_business_requirements.json çš„æ•°æ®
        
    Returns:
        Document åˆ—è¡¨
    """
    documents = []
    
    for key, value in data.items():
        # åˆ›å»º Document å¯¹è±¡
        doc = Document(
            page_content=value.get("business_requirement", ""),
            metadata={
                "id": key,
                "name": value.get("name", ""),
                "source": "query_business_requirements"
            }
        )
        documents.append(doc)
    
    print(f"âœ… ä¸šåŠ¡éœ€æ±‚çŸ¥è¯†åº“ï¼šåŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


def build_meta_tables_kb(data: dict) -> List[Document]:
    """
    æ„å»ºæ•°æ®è¡¨å…ƒæ•°æ®çŸ¥è¯†åº“
    å°†æ¯ä¸ª chunk_content ä½œä¸º chunkï¼Œid å’Œ table_name ä½œä¸ºå…ƒæ•°æ®
    
    Args:
        data: singabi_meta_tables.json çš„æ•°æ®
        
    Returns:
        Document åˆ—è¡¨
    """
    documents = []
    
    for key, value in data.items():
        # åˆ›å»º Document å¯¹è±¡
        doc = Document(
            page_content=value.get("chunk_content", ""),
            metadata={
                "id": value.get("id", key),
                "table_name": value.get("table_name", ""),
                "source": "singabi_meta_tables"
            }
        )
        documents.append(doc)
    
    print(f"âœ… æ•°æ®è¡¨å…ƒæ•°æ®çŸ¥è¯†åº“ï¼šåŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


def create_vector_store(
    documents: List[Document],
    collection_name: str,
    embeddings,
    persist_directory: str
) -> Chroma:
    """
    åˆ›å»ºå‘é‡æ•°æ®åº“
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        collection_name: é›†åˆåç§°
        embeddings: åµŒå…¥æ¨¡å‹
        persist_directory: æŒä¹…åŒ–ç›®å½•
        
    Returns:
        Chroma å‘é‡æ•°æ®åº“å®ä¾‹
    """
    print(f"ğŸ”„ æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“ï¼š{collection_name}")
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        collection_name=collection_name,
        persist_directory=persist_directory
    )
    
    print(f"âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼š{collection_name}")
    print(f"   - æ–‡æ¡£æ•°é‡: {len(documents)}")
    print(f"   - å­˜å‚¨è·¯å¾„: {persist_directory}")
    
    return vectorstore


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("å¼€å§‹æ„å»ºçŸ¥è¯†åº“...")
    print("=" * 60)
    
    # 1. æ£€æŸ¥ OpenAI API Key
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport OPENAI_API_KEY='your-api-key'")
        return
    
    # 2. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    print("\nğŸ“¦ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
    try:
        embeddings = OpenAIEmbeddings(  
            api_key=OPENAI_API_KEY,  # OpenRouter API å¯†é’¥  
            base_url=OPENAI_BASE_URL,      # OpenRouter API ç«¯ç‚¹  
            model="openai/text-embedding-3-large"        # é€‰æ‹©åµŒå…¥æ¨¡å‹  
        )
        print("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        return
    
    # 3. åˆ›å»ºå­˜å‚¨ç›®å½•
    os.makedirs(CHROMA_DB_DIR, exist_ok=True)
    
    # 4. æ„å»ºä¸šåŠ¡éœ€æ±‚æŸ¥è¯¢çŸ¥è¯†åº“
    print("\n" + "=" * 60)
    print("ã€çŸ¥è¯†åº“ 1ã€‘ä¸šåŠ¡éœ€æ±‚æŸ¥è¯¢çŸ¥è¯†åº“")
    print("=" * 60)
    
    try:
        query_data_path = os.path.join(DATA_DIR, QUERY_REQUIREMENTS_FILE)
        query_data = load_json_file(query_data_path)
        query_documents = build_query_requirements_kb(query_data)
        
        query_kb_path = os.path.join(CHROMA_DB_DIR, QUERY_KB_NAME)
        query_vectorstore = create_vector_store(
            documents=query_documents,
            collection_name=QUERY_KB_NAME,
            embeddings=embeddings,
            persist_directory=query_kb_path
        )
        
        # æµ‹è¯•æŸ¥è¯¢
        print("\nğŸ” æµ‹è¯•æŸ¥è¯¢ï¼šæŸ¥æ‰¾ä¸'æ”¾æ¬¾é‡‘é¢'ç›¸å…³çš„å†…å®¹")
        results = query_vectorstore.similarity_search("æ”¾æ¬¾é‡‘é¢ç»Ÿè®¡", k=3)
        for i, doc in enumerate(results, 1):
            print(f"\n   ç»“æœ {i}:")
            print(f"   - ID: {doc.metadata.get('id')}")
            print(f"   - Name: {doc.metadata.get('name')}")
            print(f"   - Content: {doc.page_content[:100]}...")
        
    except Exception as e:
        print(f"âŒ æ„å»ºä¸šåŠ¡éœ€æ±‚çŸ¥è¯†åº“å¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()
    
    # 5. æ„å»ºæ•°æ®è¡¨å…ƒæ•°æ®çŸ¥è¯†åº“
    print("\n" + "=" * 60)
    print("ã€çŸ¥è¯†åº“ 2ã€‘æ•°æ®è¡¨å…ƒæ•°æ®çŸ¥è¯†åº“")
    print("=" * 60)
    
    try:
        table_data_path = os.path.join(DATA_DIR, META_TABLES_FILE)
        table_data = load_json_file(table_data_path)
        table_documents = build_meta_tables_kb(table_data)
        
        table_kb_path = os.path.join(CHROMA_DB_DIR, TABLE_KB_NAME)
        table_vectorstore = create_vector_store(
            documents=table_documents,
            collection_name=TABLE_KB_NAME,
            embeddings=embeddings,
            persist_directory=table_kb_path
        )
        
        # æµ‹è¯•æŸ¥è¯¢
        print("\nğŸ” æµ‹è¯•æŸ¥è¯¢ï¼šæŸ¥æ‰¾ä¸'ç”¨æˆ·ä¿¡æ¯'ç›¸å…³çš„è¡¨")
        results = table_vectorstore.similarity_search("ç”¨æˆ·åŸºæœ¬ä¿¡æ¯è¡¨", k=3)
        for i, doc in enumerate(results, 1):
            print(f"\n   ç»“æœ {i}:")
            print(f"   - ID: {doc.metadata.get('id')}")
            print(f"   - Table: {doc.metadata.get('table_name')}")
            print(f"   - Content: {doc.page_content[:150]}...")
        
    except Exception as e:
        print(f"âŒ æ„å»ºæ•°æ®è¡¨å…ƒæ•°æ®çŸ¥è¯†åº“å¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()
    
    # 6. å®Œæˆ
    print("\n" + "=" * 60)
    print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
    print("=" * 60)
    print(f"\nğŸ“ çŸ¥è¯†åº“å­˜å‚¨ä½ç½®ï¼š")
    print(f"   1. ä¸šåŠ¡éœ€æ±‚æŸ¥è¯¢çŸ¥è¯†åº“: {os.path.join(CHROMA_DB_DIR, QUERY_KB_NAME)}")
    print(f"   2. æ•°æ®è¡¨å…ƒæ•°æ®çŸ¥è¯†åº“: {os.path.join(CHROMA_DB_DIR, TABLE_KB_NAME)}")
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•ï¼š")
    print(f"   å¯ä»¥ä½¿ç”¨ query_knowledge_base.py è„šæœ¬æŸ¥è¯¢çŸ¥è¯†åº“")


if __name__ == "__main__":
    main()
