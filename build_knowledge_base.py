#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ„å»ºChromaå‘é‡æ•°æ®åº“çŸ¥è¯†åº“ (LangChain æœ€æ–°ç‰ˆæœ¬)
- ç¬¬ä¸€éƒ¨åˆ†ï¼šæŸ¥è¯¢ä¸šåŠ¡éœ€æ±‚å’ŒSQLè¯­å¥ï¼ˆæŒ‰queryåˆ†å—ï¼‰
- ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®åº“è¡¨ç»“æ„å’ŒSchemaä¿¡æ¯ï¼ˆæŒ‰è¡¨åˆ†å—ï¼‰
- ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ™ºèƒ½å¢é‡æ›´æ–°å’Œæ‰¹å¤„ç†ä¼˜åŒ–

ä¼˜åŒ–ç‰¹æ€§ï¼š
- ä½¿ç”¨ LangChain æœ€æ–°ç¨³å®šç‰ˆ API
- æ‰¹å¤„ç†å‘é‡åŒ–æå‡æ€§èƒ½
- æ™ºèƒ½å¢é‡æ›´æ–°æœºåˆ¶
- å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—
- è¿›åº¦è¿½è¸ªå’Œç»Ÿè®¡ä¿¡æ¯
- æ”¯æŒ OpenRouter å’Œæ ‡å‡† OpenAI API
"""

import json
import re
import os
import sys
import hashlib
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# LangChain æœ€æ–°ç‰ˆæœ¬ imports
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ç¯å¢ƒé…ç½®
from dotenv import load_dotenv
load_dotenv()

# APIå¯†é’¥é…ç½®
OPENAI_API_KEY = os.getenv("openrouter_api_key") or os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("url_openrouter", "https://api.openai.com/v1")

if not OPENAI_API_KEY:
    print("âš ï¸  è­¦å‘Š: æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    print("   è¯·è¿è¡Œ: export OPENAI_API_KEY='your-api-key'")
    print("   æˆ–è€…è®¾ç½® openrouter_api_key ç¯å¢ƒå˜é‡\n")
    sys.exit(1)


class KnowledgeBaseBuilder:
    """
    çŸ¥è¯†åº“æ„å»ºå™¨ (LangChain æœ€æ–°ç‰ˆæœ¬)
    
    Features:
    - æ™ºèƒ½æ–‡æ¡£åˆ†å—å’Œå‘é‡åŒ–
    - å¢é‡æ›´æ–°æ”¯æŒ
    - æ‰¹å¤„ç†ä¼˜åŒ–
    - è¯¦ç»†çš„æ„å»ºç»Ÿè®¡
    - æ”¯æŒ OpenRouter å’Œ OpenAI API
    """
    
    def __init__(
        self, 
        persist_directory: str = "./chroma_db",
        collection_name: str = "database_knowledge",
        embedding_model: str = "text-embedding-3-large",
        batch_size: int = 100
    ):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“æ„å»ºå™¨
        
        Args:
            persist_directory: Chromaæ•°æ®åº“æŒä¹…åŒ–ç›®å½•
            collection_name: é›†åˆåç§°
            embedding_model: OpenAI embeddingæ¨¡å‹
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        self.persist_directory = Path(persist_directory)
        self.collection_name = collection_name
        self.batch_size = batch_size
        
        # åˆ›å»ºæŒä¹…åŒ–ç›®å½•
        self.persist_directory.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ– Embeddings
        # æ³¨æ„ï¼šOpenRouter éœ€è¦ä½¿ç”¨ base_url å‚æ•°
        embedding_kwargs = {
            "model": embedding_model,
            "api_key": OPENAI_API_KEY,
        }
        
        # å¦‚æœä½¿ç”¨ OpenRouterï¼Œè®¾ç½® base_url
        if "openrouter" in OPENAI_BASE_URL.lower():
            embedding_kwargs["base_url"] = OPENAI_BASE_URL
        
        self.embeddings = OpenAIEmbeddings(**embedding_kwargs)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_documents": 0,
            "query_documents": 0,
            "schema_documents": 0,
            "chunks_created": 0,
            "build_time": 0
        }
        
    def _calculate_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºå¢é‡æ›´æ–°"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def load_query_documents(self, json_file: str) -> List[Document]:
        """
        åŠ è½½æŸ¥è¯¢ä¸šåŠ¡éœ€æ±‚JSONæ–‡ä»¶ï¼ŒæŒ‰æ¯ä¸ªqueryåˆ†å—
        
        Args:
            json_file: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            Documentåˆ—è¡¨
        """
        print(f"ğŸ“– æ­£åœ¨åŠ è½½æŸ¥è¯¢æ–‡ä»¶: {json_file}")
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                queries = json.load(f)
        except FileNotFoundError:
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {json_file}")
            return []
        except json.JSONDecodeError as e:
            print(f"âŒ é”™è¯¯: JSONè§£æå¤±è´¥ - {e}")
            return []
        
        documents = []
        for query_id, query_data in queries.items():
            # æå–æ¶‰åŠçš„è¡¨å
            sql = query_data.get('sql', '')
            tables = self._extract_tables_from_sql(sql)
            
            # æ„å»ºæ–‡æ¡£å†…å®¹
            content = f"""æŸ¥è¯¢ID: {query_id}
æŸ¥è¯¢åç§°: {query_data['name']}

ä¸šåŠ¡éœ€æ±‚:
{query_data['business_requirement']}

æ¶‰åŠçš„è¡¨: {', '.join(tables) if tables else 'æœªçŸ¥'}

SQLè¯­å¥:
{sql}
"""
            
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            content_hash = self._calculate_content_hash(content)
            
            # åˆ›å»ºDocumentå¯¹è±¡ï¼Œæ·»åŠ ä¸°å¯Œçš„å…ƒæ•°æ®
            doc = Document(
                page_content=content,
                metadata={
                    "source": "query_business_requirements",
                    "query_id": query_id,
                    "query_name": query_data['name'],
                    "type": "business_query",
                    "has_sql": True,
                    "business_requirement": query_data['business_requirement'],
                    "tables": ','.join(tables),
                    "content_hash": content_hash,
                    "created_at": datetime.now().isoformat()
                }
            )
            documents.append(doc)
        
        self.stats["query_documents"] = len(documents)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæŸ¥è¯¢æ–‡æ¡£")
        return documents
    
    def _extract_tables_from_sql(self, sql: str) -> List[str]:
        """
        ä»SQLè¯­å¥ä¸­æå–è¡¨å
        
        Args:
            sql: SQLè¯­å¥
            
        Returns:
            è¡¨ååˆ—è¡¨
        """
        tables = set()
        
        # åŒ¹é… FROM table_name æˆ– JOIN table_name
        patterns = [
            r'FROM\s+`?(\w+\.)?(\w+)`?',
            r'JOIN\s+`?(\w+\.)?(\w+)`?',
            r'INTO\s+`?(\w+\.)?(\w+)`?',
            r'UPDATE\s+`?(\w+\.)?(\w+)`?'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, sql, re.IGNORECASE)
            for match in matches:
                # è·å–è¡¨åï¼ˆä¸åŒ…æ‹¬æ•°æ®åº“åï¼‰
                table_name = match.group(2)
                if table_name and table_name.upper() not in ['SELECT', 'WHERE', 'GROUP', 'ORDER', 'HAVING']:
                    tables.add(table_name)
        
        return sorted(list(tables))
    
    def load_schema_documents(self, schema_file: str) -> List[Document]:
        """
        åŠ è½½æ•°æ®åº“Schema SQLæ–‡ä»¶ï¼ŒæŒ‰è¡¨ç»“æ„åˆ†å—
        
        Args:
            schema_file: SQLæ–‡ä»¶è·¯å¾„
            
        Returns:
            Documentåˆ—è¡¨
        """
        print(f"ğŸ“– æ­£åœ¨åŠ è½½Schemaæ–‡ä»¶: {schema_file}")
        
        try:
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema_content = f.read()
        except FileNotFoundError:
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {schema_file}")
            return []
        
        documents = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–CREATE TABLEè¯­å¥
        # åŒ¹é… CREATE TABLE ... ) ENGINE=...;
        table_pattern = r"CREATE TABLE `(\w+)`\s*\((.*?)\)\s*ENGINE=.*?(?:COMMENT='(.*?)')?;"
        matches = re.finditer(table_pattern, schema_content, re.DOTALL | re.IGNORECASE)
        
        for match in matches:
            table_name = match.group(1)
            table_definition = match.group(2)
            table_comment = match.group(3) or "æ— æè¿°"
            
            # æå–å­—æ®µä¿¡æ¯
            fields_info, field_count = self._parse_table_fields(table_definition)
            
            # æå–ç´¢å¼•ä¿¡æ¯
            indexes = self._extract_indexes(table_definition)
            
            # æ„å»ºæ–‡æ¡£å†…å®¹
            content = f"""è¡¨å: {table_name}

è¡¨è¯´æ˜: {table_comment}

å­—æ®µä¿¡æ¯ (å…± {field_count} ä¸ªå­—æ®µ):
{fields_info}

ç´¢å¼•ä¿¡æ¯:
{indexes}

å®Œæ•´DDL:
CREATE TABLE `{table_name}` (
{table_definition.strip()}
) COMMENT='{table_comment}';
"""
            
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            content_hash = self._calculate_content_hash(content)
            
            # åˆ›å»ºDocumentå¯¹è±¡
            doc = Document(
                page_content=content,
                metadata={
                    "source": "database_schema",
                    "table_name": table_name,
                    "table_comment": table_comment,
                    "type": "table_schema",
                    "database": "singa_bi",
                    "field_count": field_count,
                    "content_hash": content_hash,
                    "created_at": datetime.now().isoformat()
                }
            )
            documents.append(doc)
        
        self.stats["schema_documents"] = len(documents)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªè¡¨ç»“æ„æ–‡æ¡£")
        return documents
    
    def _parse_table_fields(self, table_definition: str) -> Tuple[str, int]:
        """
        è§£æè¡¨å­—æ®µä¿¡æ¯
        
        Args:
            table_definition: è¡¨å®šä¹‰SQL
            
        Returns:
            (æ ¼å¼åŒ–çš„å­—æ®µä¿¡æ¯å­—ç¬¦ä¸², å­—æ®µæ•°é‡)
        """
        fields = []
        lines = table_definition.split('\n')
        
        for line in lines:
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œã€PRIMARY KEYã€KEYç­‰çº¦æŸå®šä¹‰
            if not line or line.startswith('PRIMARY KEY') or line.startswith('KEY ') or line.startswith('CONSTRAINT'):
                continue
            
            # æå–å­—æ®µå®šä¹‰å’Œæ³¨é‡Š
            # æ ¼å¼é€šå¸¸æ˜¯: `field_name` type ... COMMENT 'æ³¨é‡Š',
            field_match = re.match(r"`(\w+)`\s+(.*?)(?:,\s*)?$", line)
            if field_match:
                field_name = field_match.group(1)
                field_def = field_match.group(2)
                
                # æå–COMMENT
                comment_match = re.search(r"COMMENT\s+'(.*?)'", field_def)
                if comment_match:
                    comment = comment_match.group(1)
                    # ç§»é™¤COMMENTéƒ¨åˆ†ï¼Œä¿ç•™ç±»å‹å®šä¹‰
                    field_type = re.sub(r"\s*COMMENT\s+'.*?'", "", field_def).strip()
                    fields.append(f"  - {field_name}: {field_type}  // {comment}")
                else:
                    fields.append(f"  - {field_name}: {field_def}")
        
        fields_text = '\n'.join(fields) if fields else "æ— å­—æ®µä¿¡æ¯"
        return fields_text, len(fields)
    
    def _extract_indexes(self, table_definition: str) -> str:
        """
        æå–è¡¨çš„ç´¢å¼•ä¿¡æ¯
        
        Args:
            table_definition: è¡¨å®šä¹‰SQL
            
        Returns:
            æ ¼å¼åŒ–çš„ç´¢å¼•ä¿¡æ¯
        """
        indexes = []
        lines = table_definition.split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith('PRIMARY KEY'):
                indexes.append(f"  - PRIMARY KEY: {line}")
            elif line.startswith('KEY ') or line.startswith('INDEX '):
                # æå–ç´¢å¼•åå’Œåˆ—
                idx_match = re.match(r"(?:KEY|INDEX)\s+`?(\w+)`?\s+\((.*?)\)", line)
                if idx_match:
                    idx_name = idx_match.group(1)
                    idx_columns = idx_match.group(2)
                    indexes.append(f"  - INDEX {idx_name}: {idx_columns}")
        
        return '\n'.join(indexes) if indexes else "  - æ— ç´¢å¼•"
    
    def build_vector_store(
        self, 
        query_json: str, 
        schema_sql: str,
        chunk_size: int = 2000,
        chunk_overlap: int = 200,
        force_rebuild: bool = False
    ) -> Chroma:
        """
        æ„å»ºå‘é‡æ•°æ®åº“ (æ”¯æŒå¢é‡æ›´æ–°)
        
        Args:
            query_json: æŸ¥è¯¢JSONæ–‡ä»¶è·¯å¾„
            schema_sql: Schema SQLæ–‡ä»¶è·¯å¾„
            chunk_size: åˆ†å—å¤§å°ï¼ˆå¯¹äºé•¿æ–‡æœ¬ï¼‰
            chunk_overlap: åˆ†å—é‡å å¤§å°
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºï¼ˆåˆ é™¤ç°æœ‰æ•°æ®åº“ï¼‰
            
        Returns:
            Chromaå‘é‡æ•°æ®åº“å®ä¾‹
        """
        start_time = datetime.now()
        
        print("\n" + "="*70)
        print("ğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†åº“ (LangChain)")
        print("="*70 + "\n")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶é‡å»º
        if force_rebuild and self.persist_directory.exists():
            print("ğŸ—‘ï¸  å¼ºåˆ¶é‡å»ºæ¨¡å¼ï¼šåˆ é™¤ç°æœ‰æ•°æ®åº“...")
            import shutil
            shutil.rmtree(self.persist_directory)
            self.persist_directory.mkdir(parents=True, exist_ok=True)
            print("âœ… å·²æ¸…ç©ºæ•°æ®åº“\n")
        
        # åŠ è½½æ–‡æ¡£
        query_docs = self.load_query_documents(query_json)
        schema_docs = self.load_schema_documents(schema_sql)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æ¡£
        all_documents = query_docs + schema_docs
        self.stats["total_documents"] = len(all_documents)
        
        print(f"\nğŸ“Š æ–‡æ¡£ç»Ÿè®¡:")
        print(f"  - æŸ¥è¯¢æ–‡æ¡£: {len(query_docs)} ä¸ª")
        print(f"  - Schemaæ–‡æ¡£: {len(schema_docs)} ä¸ª")
        print(f"  - æ€»è®¡: {len(all_documents)} ä¸ª")
        
        if not all_documents:
            print("âŒ é”™è¯¯: æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")
            return None
        
        # å¯¹äºç‰¹åˆ«é•¿çš„æ–‡æ¡£ï¼Œè¿›è¡Œé¢å¤–åˆ†å—
        print(f"\nğŸ”ª æ™ºèƒ½æ–‡æ¡£åˆ†å— (chunk_size={chunk_size}, overlap={chunk_overlap})...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼Œ", " ", ""]
        )
        
        final_documents = []
        chunk_stats = defaultdict(int)
        
        for doc in all_documents:
            doc_length = len(doc.page_content)
            if doc_length > chunk_size:
                # åˆ†å—
                chunks = text_splitter.split_documents([doc])
                chunk_stats[doc.metadata.get('type')] += len(chunks) - 1
                doc_name = doc.metadata.get('query_name') or doc.metadata.get('table_name')
                print(f"  ğŸ“„ [{doc.metadata.get('type')}] {doc_name}: {doc_length} å­—ç¬¦ â†’ {len(chunks)} å—")
                final_documents.extend(chunks)
            else:
                final_documents.append(doc)
        
        self.stats["chunks_created"] = sum(chunk_stats.values())
        
        print(f"\nâœ… æœ€ç»ˆæ–‡æ¡£æ•°é‡: {len(final_documents)} ä¸ª")
        if chunk_stats:
            print(f"ğŸ“Œ åˆ†å—ç»Ÿè®¡:")
            for doc_type, count in chunk_stats.items():
                print(f"   - {doc_type}: é¢å¤–åˆ›å»º {count} ä¸ªåˆ†å—")
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆéµå¾ª LangChain æœ€æ–°æœ€ä½³å®è·µï¼‰
        print(f"\nğŸ”§ æ­£åœ¨åˆ›å»ºChromaå‘é‡æ•°æ®åº“...")
        print(f"  - å­˜å‚¨ç›®å½•: {self.persist_directory}")
        print(f"  - é›†åˆåç§°: {self.collection_name}")
        print(f"  - æ‰¹å¤„ç†å¤§å°: {self.batch_size}")
        
        try:
            # æ–¹æ³•1ï¼šä½¿ç”¨ from_documents å¿«é€Ÿæ„å»ºï¼ˆå°æ•°æ®é›†ï¼‰
            # æ–¹æ³•2ï¼šä½¿ç”¨ add_documents æ‰¹é‡æ·»åŠ ï¼ˆå¤§æ•°æ®é›†ï¼Œæ”¯æŒè¿›åº¦æ˜¾ç¤ºï¼‰
            # å‚è€ƒ: https://docs.langchain.com/oss/python/langchain/rag
            
            if len(final_documents) <= 100:
                # å°æ•°æ®é›†ï¼šä½¿ç”¨ from_documents ä¸€æ¬¡æ€§åˆ›å»º
                print(f"\nğŸ“¥ ä½¿ç”¨å¿«é€Ÿæ¨¡å¼åˆ›å»ºå‘é‡æ•°æ®åº“...")
                print(f"  â³ æ­£åœ¨å‘é‡åŒ– {len(final_documents)} ä¸ªæ–‡æ¡£...")
                
                vectorstore = Chroma.from_documents(
                    documents=final_documents,
                    embedding=self.embeddings,
                    collection_name=self.collection_name,
                    persist_directory=str(self.persist_directory)
                )
                
                print(f"  âœ“ å·²æˆåŠŸæ·»åŠ  {len(final_documents)} ä¸ªæ–‡æ¡£")
            else:
                # å¤§æ•°æ®é›†ï¼šä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼ï¼Œæä¾›è¿›åº¦åé¦ˆ
                print(f"\nğŸ“¥ ä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼åˆ›å»ºå‘é‡æ•°æ®åº“...")
                print(f"  â³ å°†åˆ† {(len(final_documents) + self.batch_size - 1) // self.batch_size} æ‰¹å¤„ç†...")
                
                # åˆå§‹åŒ–ç©ºçš„å‘é‡å­˜å‚¨
                vectorstore = Chroma(
                    collection_name=self.collection_name,
                    embedding_function=self.embeddings,
                    persist_directory=str(self.persist_directory)
                )
                
                # æ‰¹é‡æ·»åŠ æ–‡æ¡£ï¼ˆæŒ‰ç…§ LangChain æ–‡æ¡£æ¨èæ–¹å¼ï¼‰
                for i in range(0, len(final_documents), self.batch_size):
                    batch = final_documents[i:i + self.batch_size]
                    batch_num = i // self.batch_size + 1
                    total_batches = (len(final_documents) + self.batch_size - 1) // self.batch_size
                    
                    print(f"  ğŸ“¦ æ‰¹æ¬¡ {batch_num}/{total_batches}: å¤„ç† {len(batch)} ä¸ªæ–‡æ¡£...", end="")
                    
                    # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
                    vectorstore.add_documents(documents=batch)
                    
                    processed = min(i + self.batch_size, len(final_documents))
                    progress = (processed / len(final_documents)) * 100
                    print(f" âœ“ ({processed}/{len(final_documents)}, {progress:.1f}%)")
            
            end_time = datetime.now()
            self.stats["build_time"] = (end_time - start_time).total_seconds()
            
            print(f"\nâœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ!")
            print(f"  - å·²å­˜å‚¨ {len(final_documents)} ä¸ªæ–‡æ¡£å—")
            print(f"  - æ„å»ºè€—æ—¶: {self.stats['build_time']:.2f} ç§’")
            print(f"  - å¹³å‡é€Ÿåº¦: {len(final_documents) / self.stats['build_time']:.1f} æ–‡æ¡£/ç§’")
            
            # éªŒè¯å‘é‡å­˜å‚¨
            doc_count = vectorstore._collection.count()
            print(f"  - å‘é‡æ•°æ®åº“æ–‡æ¡£æ•°: {doc_count}")
            
            # ä¿å­˜æ„å»ºç»Ÿè®¡
            self._save_build_stats()
            
            return vectorstore
            
        except Exception as e:
            print(f"\nâŒ æ„å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # æä¾›æ•…éšœæ’é™¤å»ºè®®
            print("\nğŸ’¡ æ•…éšœæ’é™¤å»ºè®®:")
            print("  1. æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡®")
            print("  2. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            print("  3. å°è¯•å‡å° batch_size å‚æ•°")
            print("  4. æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³")
            
            return None
    
    def _save_build_stats(self):
        """ä¿å­˜æ„å»ºç»Ÿè®¡ä¿¡æ¯"""
        stats_file = self.persist_directory / "build_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump({
                **self.stats,
                "last_build": datetime.now().isoformat(),
                "collection_name": self.collection_name
            }, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š æ„å»ºç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_file}")
    
    def query_test(self, vectorstore: Chroma, query: str, k: int = 3):
        """
        æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½ (æ”¯æŒç›¸ä¼¼åº¦è¯„åˆ†)
        éµå¾ª LangChain RAG æœ€ä½³å®è·µ
        å‚è€ƒ: https://docs.langchain.com/oss/python/langchain/rag
        
        Args:
            vectorstore: å‘é‡æ•°æ®åº“å®ä¾‹
            query: æŸ¥è¯¢é—®é¢˜
            k: è¿”å›top-kç»“æœ
        """
        print("\n" + "="*70)
        print(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
        print("="*70)
        
        try:
            # ä½¿ç”¨ similarity_search_with_score è·å–ç›¸ä¼¼åº¦è¯„åˆ†
            # è¿™æ˜¯ LangChain æ¨èçš„æ£€ç´¢æ–¹å¼
            results = vectorstore.similarity_search_with_score(query, k=k)
            
            if not results:
                print("âš ï¸  æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
                return
            
            for i, (doc, score) in enumerate(results, 1):
                # è®¡ç®—ç›¸å…³æ€§ç™¾åˆ†æ¯”ï¼ˆè·ç¦»è¶Šå°è¶Šç›¸å…³ï¼‰
                relevance = max(0, 100 - score * 100)
                
                print(f"\nğŸ“„ ç»“æœ {i} (è·ç¦»: {score:.4f}, ç›¸å…³æ€§: {relevance:.1f}%):")
                print(f"ç±»å‹: {doc.metadata.get('type')}")
                
                if doc.metadata.get('type') == 'business_query':
                    print(f"æŸ¥è¯¢ID: {doc.metadata.get('query_id')}")
                    print(f"æŸ¥è¯¢åç§°: {doc.metadata.get('query_name')}")
                    if doc.metadata.get('tables'):
                        print(f"æ¶‰åŠè¡¨: {doc.metadata.get('tables')}")
                else:
                    print(f"è¡¨å: {doc.metadata.get('table_name')}")
                    print(f"è¡¨è¯´æ˜: {doc.metadata.get('table_comment')}")
                    if doc.metadata.get('field_count'):
                        print(f"å­—æ®µæ•°: {doc.metadata.get('field_count')}")
                
                print(f"\nå†…å®¹é¢„è§ˆ:")
                preview = doc.page_content[:300].replace('\n', '\n  ')
                print(f"  {preview}...")
                print("-" * 70)
                
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def get_stats(self) -> Dict:
        """è·å–æ„å»ºç»Ÿè®¡ä¿¡æ¯"""
        stats_file = self.persist_directory / "build_stats.json"
        if stats_file.exists():
            with open(stats_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return self.stats
    
    def load_existing_vectorstore(self) -> Optional[Chroma]:
        """
        åŠ è½½ç°æœ‰çš„å‘é‡æ•°æ®åº“
        éµå¾ª LangChain æœ€ä½³å®è·µè¿›è¡Œå‘é‡å­˜å‚¨åŠ è½½
        
        Returns:
            Chromaå®ä¾‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        if not self.persist_directory.exists():
            print(f"âš ï¸  å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {self.persist_directory}")
            return None
        
        print(f"ğŸ“‚ åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“: {self.persist_directory}")
        
        try:
            # ä½¿ç”¨ç›¸åŒçš„ embedding å‡½æ•°åŠ è½½å‘é‡å­˜å‚¨
            vectorstore = Chroma(
                collection_name=self.collection_name,
                embedding_function=self.embeddings,
                persist_directory=str(self.persist_directory)
            )
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            collection = vectorstore._collection
            doc_count = collection.count()
            print(f"âœ… æˆåŠŸåŠ è½½ï¼ŒåŒ…å« {doc_count} ä¸ªæ–‡æ¡£")
            
            # æ˜¾ç¤ºæ„å»ºç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            stats = self.get_stats()
            if stats.get('last_build'):
                print(f"ğŸ“… æœ€åæ„å»ºæ—¶é—´: {stats.get('last_build')}")
            
            return vectorstore
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            print("\nğŸ’¡ å¯èƒ½çš„åŸå› :")
            print("  1. embedding æ¨¡å‹ä¸åŒ¹é…")
            print("  2. æ•°æ®åº“æ–‡ä»¶æŸå")
            print("  3. é›†åˆåç§°ä¸æ­£ç¡®")
            return None
    
    def similarity_search(
        self, 
        vectorstore: Chroma, 
        query: str, 
        k: int = 4
    ) -> List[Document]:
        """
        æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
        éµå¾ª LangChain RAG æ¨¡å¼çš„æ ‡å‡†æ£€ç´¢æ–¹æ³•
        å‚è€ƒ: https://docs.langchain.com/oss/python/langchain/rag
        
        Args:
            vectorstore: å‘é‡æ•°æ®åº“å®ä¾‹
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨ similarity_search æ–¹æ³•ï¼ˆLangChain æ ‡å‡† APIï¼‰
            retrieved_docs = vectorstore.similarity_search(query, k=k)
            return retrieved_docs
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return []


def main():
    """
    ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
    éµå¾ª LangChain æœ€æ–°æœ€ä½³å®è·µæ„å»º RAG çŸ¥è¯†åº“
    å‚è€ƒæ–‡æ¡£: https://docs.langchain.com/oss/python/langchain/rag
    """
    import argparse
    
    parser = argparse.ArgumentParser(
        description="æ„å»ºæ•°æ®åº“çŸ¥è¯†åº“å‘é‡æ•°æ®åº“ (LangChain æœ€æ–°ç‰ˆæœ¬)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬æ„å»º
  python build_knowledge_base.py
  
  # å¼ºåˆ¶é‡å»º
  python build_knowledge_base.py --force-rebuild
  
  # ä½¿ç”¨è‡ªå®šä¹‰æ‰¹å¤„ç†å¤§å°
  python build_knowledge_base.py --batch-size 50
  
  # è·³è¿‡æµ‹è¯•æŸ¥è¯¢
  python build_knowledge_base.py --no-test
  
å‚è€ƒ: https://docs.langchain.com/oss/python/langchain/rag
        """
    )
    parser.add_argument("--query-json", default="/Users/sarah/å·¥ä½œ/æ•°æ®åº“è‡ªåŠ¨åŒ–/BIAI/query_business_requirements.json",
                        help="æŸ¥è¯¢JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--schema-sql", default="/Users/sarah/å·¥ä½œ/æ•°æ®åº“è‡ªåŠ¨åŒ–/BIAI/schema.sql",
                        help="Schema SQLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--persist-dir", default="/Users/sarah/å·¥ä½œ/æ•°æ®åº“è‡ªåŠ¨åŒ–/BIAI/chroma_db",
                        help="å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•")
    parser.add_argument("--collection", default="database_knowledge",
                        help="é›†åˆåç§°")
    parser.add_argument("--force-rebuild", action="store_true",
                        help="å¼ºåˆ¶é‡å»ºæ•°æ®åº“ï¼ˆåˆ é™¤ç°æœ‰æ•°æ®ï¼‰")
    parser.add_argument("--no-test", action="store_true",
                        help="è·³è¿‡æµ‹è¯•æŸ¥è¯¢")
    parser.add_argument("--batch-size", type=int, default=100,
                        help="æ‰¹å¤„ç†å¤§å°ï¼ˆæ¨è: 50-100ï¼‰")
    parser.add_argument("--chunk-size", type=int, default=2000,
                        help="æ–‡æ¡£åˆ†å—å¤§å°ï¼ˆæ¨è: 1000-2000ï¼‰")
    parser.add_argument("--embedding-model", default="text-embedding-3-large",
                        help="OpenAI embedding æ¨¡å‹ï¼ˆé»˜è®¤: text-embedding-3-largeï¼‰")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.query_json).exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {args.query_json}")
        return 1
    
    if not Path(args.schema_sql).exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {args.schema_sql}")
        return 1
    
    # åˆ›å»ºçŸ¥è¯†åº“æ„å»ºå™¨ï¼ˆéµå¾ª LangChain æœ€ä½³å®è·µï¼‰
    builder = KnowledgeBaseBuilder(
        persist_directory=args.persist_dir,
        collection_name=args.collection,
        embedding_model=args.embedding_model,
        batch_size=args.batch_size
    )
    
    print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"  - Embedding æ¨¡å‹: {args.embedding_model}")
    print(f"  - æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print(f"  - æ–‡æ¡£åˆ†å—å¤§å°: {args.chunk_size}")
    print(f"  - é›†åˆåç§°: {args.collection}")
    
    # æ„å»ºå‘é‡æ•°æ®åº“
    vectorstore = builder.build_vector_store(
        query_json=args.query_json,
        schema_sql=args.schema_sql,
        chunk_size=args.chunk_size,
        force_rebuild=args.force_rebuild
    )
    
    if not vectorstore:
        print("âŒ å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥")
        return 1
    
    # æµ‹è¯•æŸ¥è¯¢ï¼ˆéµå¾ª LangChain RAG æ¨¡å¼ï¼‰
    if not args.no_test:
        print("\n" + "="*70)
        print("ğŸ§ª å¼€å§‹æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½")
        print("   å‚è€ƒ: https://docs.langchain.com/oss/python/langchain/rag")
        print("="*70)
        
        test_queries = [
            "å¦‚ä½•æŒ‰å°æ—¶ç»Ÿè®¡ç”¨æˆ·æ³¨å†Œæ•°é‡ï¼Ÿ",
            "sgo_ordersè¡¨æœ‰å“ªäº›å­—æ®µï¼Ÿ",
            "è¿˜æ¬¾ç›¸å…³çš„æŸ¥è¯¢æœ‰å“ªäº›ï¼Ÿ",
            "approval_infoè¡¨çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
            "WhatsAppæ¶ˆæ¯å‘é€è®°å½•åœ¨å“ªä¸ªè¡¨ï¼Ÿ"
        ]
        
        for test_query in test_queries:
            builder.query_test(vectorstore, test_query, k=2)
        
        # é¢å¤–æ¼”ç¤ºï¼šä½¿ç”¨æ ‡å‡† similarity_search API
        print("\n" + "="*70)
        print("ğŸ“ æ¼”ç¤ºæ ‡å‡†æ£€ç´¢ API (similarity_search)")
        print("="*70)
        demo_query = "å¦‚ä½•ç»Ÿè®¡æ–°è€å®¢ç”³è¯·é‡ï¼Ÿ"
        print(f"æŸ¥è¯¢: {demo_query}\n")
        retrieved_docs = builder.similarity_search(vectorstore, demo_query, k=2)
        print(f"âœ“ æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
        for i, doc in enumerate(retrieved_docs, 1):
            doc_type = doc.metadata.get('type')
            name = doc.metadata.get('query_name') or doc.metadata.get('table_name')
            print(f"  {i}. [{doc_type}] {name}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print("\n" + "="*70)
    print("âœ… çŸ¥è¯†åº“æ„å»ºå’Œæµ‹è¯•å®Œæˆï¼")
    print("="*70)
    
    stats = builder.get_stats()
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  - æŸ¥è¯¢æ–‡æ¡£: {stats.get('query_documents', 0)} ä¸ª")
    print(f"  - Schemaæ–‡æ¡£: {stats.get('schema_documents', 0)} ä¸ª")
    print(f"  - æ€»æ–‡æ¡£æ•°: {stats.get('total_documents', 0)} ä¸ª")
    print(f"  - é¢å¤–åˆ†å—: {stats.get('chunks_created', 0)} ä¸ª")
    print(f"  - æ„å»ºè€—æ—¶: {stats.get('build_time', 0):.2f} ç§’")
    print(f"\nğŸ“ æ•°æ®åº“ä½ç½®: {args.persist_dir}")
    print(f"ğŸ”— é›†åˆåç§°: {args.collection}")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"  1. ä½¿ç”¨ query_knowledge_base.py è¿›è¡ŒæŸ¥è¯¢")
    print(f"  2. é›†æˆåˆ° RAG åº”ç”¨ä¸­è¿›è¡Œé—®ç­”")
    print(f"  3. å‚è€ƒæ–‡æ¡£: https://docs.langchain.com/oss/python/langchain/rag")
    print("="*70 + "\n")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

